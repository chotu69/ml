{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "7)\tImplement a Support Vector Machine model for the built-in iris dataset"
      ],
      "metadata": {
        "id": "KY-_pixRTcfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machine (SVM) is a supervised machine learning algorithm\n",
        "used for both classification and regression. Though we say regression problems as\n",
        "well its best suited for classification. The main objective of the SVM algorithm is to\n",
        "find the optimal hyper plane in an N-dimensional space that can separate the data\n",
        "points in different classes in the feature space. The hyper plane tries that the margin\n",
        "between the closest points of different classes should be as maximum as possible.\n",
        "The dimension of the hyper plane depends upon the number of features. If the\n",
        "number of input features is two, then the hyper plane is just a line. If the number of\n",
        "input features is three, then the hyper plane becomes a 2-D plane. It becomes\n",
        "difficult to imagine when the number of features exceeds three. Let’s consider two\n",
        "independent variables x1, x2, and one dependent variable which is either a blue\n",
        "circle or a red circle.\n",
        "\n",
        "Hyper plane: Hyper plane is the decision boundary that is used to separate the data points of\n",
        "different classes in a feature space. In the case of linear classifications, it will be a linear\n",
        "equation i.e. wx+b = 0.\n",
        "\n",
        "Support Vectors: These are the points that are closest to the hyperplane. A separating line\n",
        "will be defined with the help of these data points\n",
        "Margin: it is the distance between the hyperplane and the observations closest to the\n",
        "hyperplane (support vectors). In SVM large margin is considered a good margin. There are\n",
        "two types of margins : hard margin and soft margin.\n",
        "What does SVM do?\n",
        "Given a set of training examples, each marked as belonging to one or the other of two\n",
        "categories, an SVM training algorithm builds a model that assigns new examples to one\n",
        "category or the other, making it a non-probabilistic binary linear classifier.\n",
        "Pros and Cons-\n",
        "● Pros:\n",
        "○ It works really well with a clear margin of separation\n",
        "○ It is effective in high dimensional spaces and in cases where the number of\n",
        "dimensions is greater than the number of samples.\n",
        "○ It uses a subset of training points in the decision function (called support\n",
        "vectors), so it is also memory efficient.\n",
        "\n",
        "● Cons:\n",
        "○ It doesn’t perform well when we have large data set (as the required training\n",
        "time is higher) and when the data set has more noise i.e. target classes are\n",
        "overlapping\n",
        "○ SVM doesn’t directly provide probability estimates, these are calculated using\n",
        "an expensive five-fold cross-validation."
      ],
      "metadata": {
        "id": "O2Xa_QTcWKkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# i. Data scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# ii. Training and testing of the model\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# iii. Create the SVM model\n",
        "model = SVC(kernel='rbf', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# iv. Display confusion matrix and classification report\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=iris.target_names,\n",
        "            yticklabels=iris.target_names)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# v. Display k-fold cross-validation score\n",
        "cv_scores = cross_val_score(model, X_scaled, y, cv=5)\n",
        "print(\"\\nCross-validation scores:\", cv_scores)\n",
        "print(\"Mean CV score:\", cv_scores.mean())\n",
        "print(\"Standard deviation of CV scores:\", cv_scores.std())\n",
        "\n",
        "# Function to plot decision boundaries\n",
        "def plot_decision_boundaries(X, y, ax=None):\n",
        "    model = SVC(kernel='rbf', random_state=42)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    h = .02  # step size in the mesh\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "\n",
        "    ax.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n",
        "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolor='black')\n",
        "    ax.set_xlabel(iris.feature_names[0])\n",
        "    ax.set_ylabel(iris.feature_names[1])\n",
        "    return scatter\n",
        "\n",
        "# Visualize decision boundaries for different feature pairs\n",
        "fig, axs = plt.subplots(3, 2, figsize=(15, 20))\n",
        "feature_pairs = [(0, 1), (0, 2), ( 0, 3), (1, 2), (1, 3), (2, 3)]\n",
        "for i, (ax, pair) in enumerate(zip(axs.flatten(), feature_pairs)):\n",
        "    X_pair = X_scaled[:, pair]\n",
        "    scatter = plot_decision_boundaries(X_pair, y, ax=ax)\n",
        "    ax.set_title(f'Decision Boundaries (Features {pair[0]} and {pair[1]})')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uXvKo7OaTdvm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
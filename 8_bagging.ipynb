{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "8)\tImplement a Bagging model for the built-in wine dataset"
      ],
      "metadata": {
        "id": "KY-_pixRTcfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, for building solid and reliable models prediction accuracy is the key factor. Ensemble learning is a supervised machine-learning technique that combines multiple models to build a more powerful and robust model. The idea is that by combining the strengths of multiple models, we can create a model that is more robust and less likely to overfit the data. It can be used for both classifications and regression tasks.\n",
        "Ensemble learning techniques can be categorized in three ways:\n",
        "Bagging (Bootstrap Aggregating)\n",
        "Boosting\n",
        "Stacking (Stacked Generalization)\n",
        "Bagging is a supervised machine-learning technique, and it can be used for both regression and classification tasks. In this article we will discuss the bagging classifier.\n",
        "Bagging Classifier\n",
        "Bagging (or Bootstrap aggregating) is a type of ensemble learning in which multiple base models are trained independently in parallel on different subsets of the training data. Each subset is generated using bootstrap sampling, in which data points are picked at random with replacement. In the case of the Bagging classifier, the final prediction is made by aggregating the predictions of the all-base model, using majority voting. In the case of regression, the final prediction is made by averaging the predictions of the all-base model, and that is known as bagging regression.\n",
        "\n",
        "Bagging Classifier\n",
        "Bagging helps improve accuracy and reduce overfitting, especially in models that have high variance.\n",
        "Working of Bagging Classifier :\n",
        "Bootstrap Sampling: In Bootstrap Sampling randomly ‘n’ subsets of original training data are sampled with replacement. This step ensures that the base models are trained on diverse subsets of the data, as some samples may appear multiple times in the new subset, while others may be omitted. It reduces the risks of overfitting and improves the accuracy of the model.\n",
        "\n",
        "Let's break it down step by step:\n",
        "Original training dataset: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "Resampled training set 1: [2, 3, 3, 5, 6, 1, 8, 10, 9, 1]\n",
        "Resampled training set 2: [1, 1, 5, 6, 3, 8, 9, 10, 2, 7]\n",
        "Resampled training set 3: [1, 5, 8, 9, 2, 10, 9, 7, 5, 4]\n",
        "Base Model Training: In bagging, multiple base models are used. After the Bootstrap Sampling, each base model is independently trained using a specific learning algorithm, such as decision trees, support vector machines, or neural networks on a different bootstrapped subset of data. These models are typically called “Weak learners” because they may not be highly accurate on their own. Since the base model is trained independently of different subsets of data. To make the model computationally efficient and less time-consuming, the base models can be trained in parallel.\n",
        "Aggregation: Once all the base models are trained, it is used to make predictions on the unseen data i.e. the subset of data on which that base model is not trained. In the bagging classifier, the predicted class label for the given instance is chosen based on the majority voting. The class which has the majority voting is the prediction of the model.\n",
        "Out-of-Bag (OOB) Evaluation: Some samples are excluded from the training subset of particular base models during the bootstrapping method. These “out-of-bag” samples can be used to estimate the model’s performance without the need for cross-validation.\n",
        "Final Prediction: After aggregating the predictions from all the base models, Bagging produces a final prediction for each instance."
      ],
      "metadata": {
        "id": "O2Xa_QTcWKkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# i. Data scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# ii. Training and testing of the model\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# iii. Create the Bagging model\n",
        "base_estimator = DecisionTreeClassifier(random_state=42)\n",
        "try:\n",
        "    # For newer versions of scikit-learn\n",
        "    model = BaggingClassifier(estimator=base_estimator, n_estimators=10, random_state=42)\n",
        "except TypeError:\n",
        "    # For older versions of scikit-learn\n",
        "    model = BaggingClassifier(base_estimator=base_estimator, n_estimators=10, random_state=42)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# iv. Display confusion matrix and classification report\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=wine.target_names))\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=wine.target_names,\n",
        "            yticklabels=wine.target_names)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# v. Display k-fold cross-validation score\n",
        "cv_scores = cross_val_score(model, X_scaled, y, cv=5)\n",
        "print(\"\\nCross-validation scores:\", cv_scores)\n",
        "print(\"Mean CV score:\", cv_scores.mean())\n",
        "print(\"Standard deviation of CV scores:\", cv_scores.std())\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = np.mean([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
        "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
        "sorted_idx = np.argsort(feature_importance)\n",
        "pos = np.arange(sorted_idx.shape[0]) + .5\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
        "plt.yticks(pos, np.array(wine.feature_names)[sorted_idx])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.title('Variable Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uXvKo7OaTdvm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
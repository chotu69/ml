{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "9) Implement a PCA to visualize the built-in wine dataset."
      ],
      "metadata": {
        "id": "KY-_pixRTcfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used to analyze and visualize high-dimensional data. It transforms a set of correlated variables into a smaller set of uncorrelated variables called principal components. These components capture the maximum variance in the data, allowing for an easier interpretation of the structure while preserving most of the relevant information.\n",
        "\n",
        "Key Concepts in PCA:\n",
        "Variance and Covariance:\n",
        "\n",
        "Variance measures the spread of data along a particular axis.\n",
        "Covariance indicates how much two variables change together. If variables are highly correlated, PCA can transform them into a set of uncorrelated components.\n",
        "Eigenvectors and Eigenvalues:\n",
        "\n",
        "PCA computes eigenvectors and eigenvalues from the covariance matrix of the data.\n",
        "The eigenvectors define the directions of the new feature space (the principal components), while the eigenvalues describe the amount of variance captured by each principal component.\n",
        "The principal components are ordered by their eigenvalues, meaning the first principal component captures the most variance, the second captures the second most, and so on.\n",
        "Dimensionality Reduction:\n",
        "\n",
        "By keeping only the first few principal components (those with the highest variance), PCA reduces the number of dimensions of the dataset, simplifying the data for visualization or further analysis.\n",
        "Orthogonality of Principal Components:\n",
        "\n",
        "Each principal component is orthogonal (perpendicular) to the others, ensuring that the new components are uncorrelated.\n",
        "Feature Importance:\n",
        "\n",
        "The components can be used to determine which original features contribute most to the variance in the data."
      ],
      "metadata": {
        "id": "O2Xa_QTcWKkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Load the wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Calculate the explained variance ratio\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# Plot the cumulative explained variance ratio\n",
        "plt.figure(figsize=(10, 6))\n",
        "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
        "plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, 'bo-')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "plt.title('Cumulative Explained Variance Ratio vs. Number of Components')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2D visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "colors = ['r', 'g', 'b']\n",
        "for color, i, target_name in zip(colors, [0, 1, 2], wine.target_names):\n",
        "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], color=color, alpha=.8, lw=2,\n",
        "                label=target_name)\n",
        "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
        "plt.title('PCA of Wine Dataset (2 components)')\n",
        "plt.xlabel(f'First Principal Component ({explained_variance_ratio[0]:.2f})')\n",
        "plt.ylabel(f'Second Principal Component ({explained_variance_ratio[1]:.2f})')\n",
        "plt.show()\n",
        "\n",
        "# 3D visualization\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "for color, i, target_name in zip(colors, [0, 1, 2], wine.target_names):\n",
        "    ax.scatter(X_pca[y == i, 0], X_pca[y == i, 1], X_pca[y == i, 2], color=color, alpha=.8,\n",
        "               label=target_name)\n",
        "ax.legend(loc='best', shadow=False, scatterpoints=1)\n",
        "ax.set_title('PCA of Wine Dataset (3 components)')\n",
        "ax.set_xlabel(f'First Principal Component ({explained_variance_ratio[0]:.2f})')\n",
        "ax.set_ylabel(f'Second Principal Component ({explained_variance_ratio[1]:.2f})')\n",
        "ax.set_zlabel(f'Third Principal Component ({explained_variance_ratio[2]:.2f})')\n",
        "plt.show()\n",
        "\n",
        "# Print the explained variance ratio for each component\n",
        "for i, ratio in enumerate(explained_variance_ratio):\n",
        "    print(f\"PC{i+1} explained variance ratio: {ratio:.4f}\")\n",
        "\n",
        "# Print the cumulative explained variance ratio for 2 and 3 components\n",
        "print(f\"\\nCumulative explained variance ratio (2 components): {cumulative_variance_ratio[1]:.4f}\")\n",
        "print(f\"Cumulative explained variance ratio (3 components): {cumulative_variance_ratio[2]:.4f}\")"
      ],
      "metadata": {
        "id": "uXvKo7OaTdvm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}